{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wCu0rLyra_bD"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeout\n",
        "import re\n",
        "import os\n",
        "from typing import Dict\n",
        "from openai import AzureOpenAI\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# --- Parámetros de Azure OpenAI ---\n",
        "ENDPOINT   = \"https://fausp-mbmvwtiw-eastus2.cognitiveservices.azure.com/\"\n",
        "API_KEY    = \"8U02J0d4710ZcPDqs9J6cWj7l1CDWKv8Yg8sWRO4eLwLEtsIOfDSJQQJ99BFACHYHv6XJ3w3AAAAACOGRHrr\"\n",
        "DEPLOYMENT = \"gpt-4o-mini-faus\"\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    api_version=\"2024-12-01-preview\",\n",
        "    azure_endpoint=ENDPOINT,\n",
        "    api_key=API_KEY\n",
        ")\n",
        "\n",
        "REMOTE_KEYWORDS = [\"remote\", \"remoto\", \"teletrabajo\", \"work from home\"]\n",
        "INACTIVE_KEYWORDS = [\n",
        "    \"no longer accepting applications\", \"job is no longer available\",\n",
        "    \"position filled\", \"job expired\", \"vacante cerrada\"\n",
        "]\n",
        "\n",
        "def extract_keywords(text, keyword_list):\n",
        "    text_lower = text.lower()\n",
        "    return any(keyword in text_lower for keyword in keyword_list)\n",
        "\n",
        "def extract_responsibilities(text):\n",
        "    lines = text.split('\\n')\n",
        "    bullets = [line.strip('-• ').strip() for line in lines if re.search(r'[-•]', line)]\n",
        "    return [b for b in bullets if len(b) > 20]\n",
        "\n",
        "def extract_skills(text):\n",
        "    skill_keywords = [\n",
        "        \"python\", \"java\", \"sql\", \"machine learning\", \"deep learning\", \"tensorflow\", \"pandas\",\n",
        "        \"numpy\", \"docker\", \"git\", \"linux\", \"cloud\", \"azure\", \"aws\", \"scikit\", \"data analysis\",\n",
        "        \"visualization\", \"nlp\", \"pytorch\", \"r\", \"power bi\", \"excel\", \"spark\"\n",
        "    ]\n",
        "    text_lower = text.lower()\n",
        "    return list({kw for kw in skill_keywords if kw in text_lower})\n",
        "\n",
        "def extract_education(text):\n",
        "    edu_keywords = [\"licenciatura\", \"grado\", \"ingeniería\", \"doctorado\", \"maestría\", \"msc\", \"bsc\"]\n",
        "    for kw in edu_keywords:\n",
        "        if kw in text.lower():\n",
        "            return kw\n",
        "    return None\n",
        "\n",
        "async def scrape_linkedin_job_async(link: str) -> Dict:\n",
        "    \"\"\"\n",
        "    Scrapea título, empresa, ubicación y descripción de una oferta LinkedIn,\n",
        "    capturando timeouts de forma segura.\n",
        "    \"\"\"\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        page = await browser.new_page()\n",
        "        await page.goto(link, timeout=20000)\n",
        "        # Espera genérica corta para h1\n",
        "        try:\n",
        "            await page.wait_for_selector(\"h1\", timeout=5000)\n",
        "        except PlaywrightTimeout:\n",
        "            logger.warning(f\"Timeout al cargar <h1> en {link}\")\n",
        "\n",
        "        # Helper para inner_text con timeout corto y captura de error\n",
        "        async def safe_inner_text(selector: str) -> str:\n",
        "            try:\n",
        "                return await page.locator(selector).inner_text(timeout=5000)\n",
        "            except PlaywrightTimeout:\n",
        "                logger.warning(f\"Timeout al buscar selector '{selector}' en {link}\")\n",
        "                return \"\"\n",
        "\n",
        "        # Scrapeo de campos\n",
        "        title    = await safe_inner_text(\"h1\")\n",
        "        company  = await safe_inner_text(\"a.topcard__org-name-link\")\n",
        "        location = await safe_inner_text(\"span.topcard__flavor--bullet\")\n",
        "\n",
        "        # Descripción (puede ser más larga)\n",
        "        try:\n",
        "            await page.wait_for_selector(\"div.description__text\", timeout=5000)\n",
        "            desc = await page.locator(\"div.description__text\").inner_text(timeout=5000)\n",
        "        except PlaywrightTimeout:\n",
        "            logger.warning(f\"Timeout al cargar descripción en {link}\")\n",
        "            desc = \"\"\n",
        "\n",
        "        await browser.close()\n",
        "\n",
        "        # Procesamiento de texto\n",
        "        is_remote   = extract_keywords(desc, REMOTE_KEYWORDS)\n",
        "        is_active   = not extract_keywords(desc, INACTIVE_KEYWORDS)\n",
        "        responsibilities = extract_responsibilities(desc)\n",
        "        skills           = extract_skills(desc)\n",
        "        education        = extract_education(desc)\n",
        "\n",
        "        return {\n",
        "            \"title\": title,\n",
        "            \"company\": company,\n",
        "            \"location\": location,\n",
        "            \"desc\": desc,\n",
        "            \"is_remote\": is_remote,\n",
        "            \"is_active\": is_active,\n",
        "            \"responsibilities\": responsibilities,\n",
        "            \"skills\": skills,\n",
        "            \"education\": education\n",
        "        }\n",
        "\n",
        "def build_summary_prompt(job: Dict) -> str:\n",
        "    return f\"\"\"\n",
        "You are an assistant that reads a job posting and produces a concise single-paragraph summary covering the key highlights.\n",
        "\n",
        "Job details:\n",
        "• Title: {job['title']}\n",
        "• Company: {job['company']}\n",
        "• Location: {job['location']}\n",
        "• Remote: {\"Yes\" if job[\"is_remote\"] else \"No\"}\n",
        "• Responsibilities: {', '.join(job['responsibilities'])}\n",
        "• Skills: {', '.join(job['skills'])}\n",
        "• Education: {job['education']}\n",
        "\n",
        "Task:\n",
        "Write one brief paragraph (2–3 sentences) that summarizes the role (including whether it’s remote), main responsibilities, required skills, and any standout detail. Use plain, professional English without bullet points.\n",
        "\"\"\".strip()\n",
        "\n",
        "def summarize_job_with_azure(job: Dict) -> str:\n",
        "    prompt = build_summary_prompt(job)\n",
        "    resp = client.chat.completions.create(\n",
        "        model=DEPLOYMENT,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\",   \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.3,\n",
        "        max_tokens=150\n",
        "    )\n",
        "    return resp.choices[0].message.content.strip()\n",
        "\n",
        "async def scrape_and_summarize(link: str) -> Dict:\n",
        "    job     = await scrape_linkedin_job_async(link)\n",
        "    summary = summarize_job_with_azure(job)\n",
        "    return {\"job\": job, \"summary\": summary}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQqoOK_RY37N",
        "outputId": "217308e4-29aa-434a-c877-0c6b0556463a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting googlesearch-python\n",
            "  Downloading googlesearch_python-1.3.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.9 in /usr/local/lib/python3.11/dist-packages (from googlesearch-python) (4.13.4)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/dist-packages (from googlesearch-python) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (4.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->googlesearch-python) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->googlesearch-python) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->googlesearch-python) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.20->googlesearch-python) (2025.6.15)\n",
            "Downloading googlesearch_python-1.3.0-py3-none-any.whl (5.6 kB)\n",
            "Installing collected packages: googlesearch-python\n",
            "Successfully installed googlesearch-python-1.3.0\n"
          ]
        }
      ],
      "source": [
        "%pip install googlesearch-python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3rgXLTrKsYWM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "import requests\n",
        "from typing import List\n",
        "from googlesearch import search   # pip install googlesearch-python\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "LANG_REMOTE_TERMS = {\n",
        "    \"es\": [\"remoto\", \"teletrabajo\"],\n",
        "    \"en\": [\"remote\"],\n",
        "    \"fr\": [\"télétravail\"],\n",
        "    \"de\": [\"Fernarbeit\", \"Homeoffice\"],\n",
        "    \"pt\": [\"remoto\", \"teletrabalho\"]\n",
        "}\n",
        "\n",
        "def build_search_query(query: str, localization: str, linkedin_only: bool = True) -> str:\n",
        "    loc_term = f'\"{localization}\"'\n",
        "    remote_terms = [t for terms in LANG_REMOTE_TERMS.values() for t in terms]\n",
        "    all_terms = set([loc_term] + [f'\"{t}\"' for t in remote_terms])\n",
        "    base = f'\"{query}\" ({\" OR \".join(all_terms)})'\n",
        "    if linkedin_only:\n",
        "        base += \" site:linkedin.com/jobs/view\"\n",
        "    return base\n",
        "\n",
        "def verify_url(url: str, session: requests.Session, timeout: float = 5.0) -> bool:\n",
        "    \"\"\"Devuelve True si la URL responde con un código HTTP < 400.\"\"\"\n",
        "    try:\n",
        "        r = session.head(url, timeout=timeout, allow_redirects=True)\n",
        "        return r.status_code < 400\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def run_search_no_api(\n",
        "    query: str,\n",
        "    localization: str,\n",
        "    linkedin_only: bool,\n",
        "    max_urls: int\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Usa googlesearch.search() para obtener URLs, luego las filtra\n",
        "    y verifica su disponibilidad HTTP.\n",
        "    \"\"\"\n",
        "    q = build_search_query(query, localization, linkedin_only)\n",
        "    session = requests.Session()\n",
        "    found, seen = [], set()\n",
        "\n",
        "    # pedimos el doble de resultados para tener margen\n",
        "    for url in search(q, num_results=max_urls * 2, lang=\"es\"):\n",
        "        if url in seen:\n",
        "            continue\n",
        "        seen.add(url)\n",
        "        if verify_url(url, session):\n",
        "            found.append(url)\n",
        "            if len(found) >= max_urls:\n",
        "                break\n",
        "\n",
        "    return found\n",
        "\n",
        "def search_jobs_multilingual(\n",
        "    query: str,\n",
        "    max_urls: int = 10,\n",
        "    localization: str = \"Buenos Aires\",\n",
        "    antiguedad_maxima: str = \"semana\",\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Busca ofertas de trabajo en varios idiomas:\n",
        "    1) LinkedIn (site:linkedin.com/jobs/view)\n",
        "    2) Fallback a toda la web\n",
        "    \"\"\"\n",
        "    # 1) Intento LinkedIn\n",
        "    urls = run_search_no_api(query, localization, linkedin_only=True, max_urls=max_urls)\n",
        "    if urls:\n",
        "        return urls\n",
        "\n",
        "    # 2) Fallback (toda la web)\n",
        "    logger.info(\"No se encontraron resultados en LinkedIn; buscando en toda la web...\")\n",
        "    return run_search_no_api(query, localization, linkedin_only=False, max_urls=max_urls)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QDflT1DTm01",
        "outputId": "6b915582-6a86-4b9a-b275-fd1aba304ec7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-5.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.0)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.1)\n",
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.33.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Downloading sentence_transformers-5.0.0-py3-none-any.whl (470 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m470.2/470.2 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.33.2-py3-none-any.whl (515 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.4/515.4 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m97.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface_hub, nvidia-cusolver-cu12, sentence-transformers\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -U sentence-transformers transformers huggingface_hub\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hu6_vQbbKOuy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import AzureOpenAI\n",
        "from serpapi import Client\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import fitz  # PyMuPDF\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from huggingface_hub import login\n",
        "import json\n",
        "import re\n",
        "import nest_asyncio\n",
        "import requests\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import asyncio\n",
        "\n",
        "# ── Configuración de Azure OpenAI ──────────────────────────────────────────────\n",
        "ENDPOINT = \"https://fausp-mbmvwtiw-eastus2.cognitiveservices.azure.com/\"\n",
        "API_KEY = \"8U02J0d4710ZcPDqs9J6cWj7l1CDWKv8Yg8sWRO4eLwLEtsIOfDSJQQJ99BFACHYHv6XJ3w3AAAAACOGRHrr\"\n",
        "DEPLOYMENT = \"gpt-4o-mini-faus\"\n",
        "\n",
        "client = AzureOpenAI(\n",
        "    api_version=\"2024-12-01-preview\",\n",
        "    azure_endpoint=ENDPOINT,\n",
        "    api_key=API_KEY\n",
        ")\n",
        "\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "You will receive a resume in raw text format. Your task is to:\n",
        "\n",
        "1. Extract the candidate’s name.\n",
        "2. Extract the candidate’s email address.\n",
        "3. Extract the candidate’s phone number.\n",
        "4. Analyze the candidate’s background (education, skills, experience, roles, industries, etc.) and craft **one concise keyword search string** (single line) for LinkedIn Jobs.\n",
        "5. Extract a cleaned and structured version of the resume that removes any personal contact information (name, email, phone, address, LinkedIn, etc.).\n",
        "\n",
        "For de fourth part:\n",
        "- Contain only the most relevant **job title(s)** plus 3-5 **key skills / industry terms**, all separated by single spaces.\n",
        "- **DO NOT** use Boolean operators (AND, OR, NOT), parentheses, quotation marks, plus signs, or any other special characters.\n",
        "- **DO NOT** include personal identifiers (name, email, phone, etc.).\n",
        "- Write it in the same language that predominates in the résumé (Spanish or English).\n",
        "- Remember: the calling function will later append `site:linkedin.com/jobs`; you only output the keywords.\n",
        "\n",
        "For the fifth part:\n",
        "- Do NOT summarize or omit key content.\n",
        "- Instead, preserve as much of the original job-related information as possible.\n",
        "- Reorganize and rephrase disconnected items into full sentences with proper structure and connectors (e.g., “The candidate worked at...”, “They were responsible for...”, “Their skills include...”).\n",
        "- You may rewrite bullet points and lists as prose, but keep all relevant details intact.\n",
        "- Do NOT include any personal identifiers or contact information.\n",
        "- Imagine you are preparing the resume for analysis by an AI model – you want to keep the full context but make it more readable.\n",
        "\n",
        "You may use the following fields **only if present** in the text:\n",
        "- Career Objective\n",
        "- Skills\n",
        "- Institution\n",
        "- Degree\n",
        "- Results\n",
        "- Field of Study\n",
        "- Companies\n",
        "- Job Skills\n",
        "- Positions\n",
        "- Responsibilities\n",
        "- Organizations\n",
        "- Roles\n",
        "- Languages\n",
        "- Proficiency\n",
        "- Certifications\n",
        "\n",
        "Respond **only** with a valid JSON object, without additional text or explanations\n",
        "\n",
        "\n",
        "Exact structure of the output:\n",
        "\n",
        "{{\n",
        "\"area_job\":\"...\",\n",
        "\"cv_information\":\"...\"\n",
        "}}\n",
        "\n",
        "CV TEXT:\n",
        "{cv_text}\n",
        "\"\"\"\n",
        "\n",
        "def safe_json_load(content: str):\n",
        "    opens = content.count('{')\n",
        "    closes = content.count('}')\n",
        "    if closes < opens:\n",
        "        content = content + '\"' + '}' * (opens - closes)\n",
        "    return json.loads(content)\n",
        "\n",
        "\n",
        "def generate_linkedin_query(cv_text: str) -> str:\n",
        "    prompt = PROMPT_TEMPLATE.format(cv_text=cv_text)\n",
        "    response = client.chat.completions.create(\n",
        "        model=DEPLOYMENT,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=0.7,\n",
        "        max_tokens=256\n",
        "    )\n",
        "    content = response.choices[0].message.content.strip()\n",
        "    if content.startswith(\"```json\"):\n",
        "            content = content.removeprefix(\"```json\").removesuffix(\"```\").strip()\n",
        "    elif content.startswith(\"```\"):\n",
        "            content = content.removeprefix(\"```\").removesuffix(\"```\").strip()\n",
        "\n",
        "    data = safe_json_load(content)\n",
        "    area_job = data.get(\"area_job\", \"\")\n",
        "    cv_information = data.get(\"cv_information\", \"\")\n",
        "\n",
        "    return [area_job, cv_information]\n",
        "\n",
        "\n",
        "# ── SerpApi Search ──────────────────────────────────────────────────────────────\n",
        "SERPAPI_API_KEYS = [\n",
        "    \"1a992f2a6dbaed0c95203a2ed73768f29b4b7f423a5f218c4834e355c5c31918\" # tizi\n",
        "    # \"e898c7f95cdb5692528a009eb2ee7d08d24a2f37c22f9623a065a96fd6072892\", # mati\n",
        "    # \"141d74f945c81589527847238881362de5f08cc31dae86209dcb2c04d7e5ccc7\", # faus\n",
        "    # \"c18acdb7b9b75162b53059cd6f094669c33323e898758841176351ac8a59e8c7\" # giaco\n",
        "]\n",
        "\n",
        "def pdf_a_string(ruta_pdf):\n",
        "    doc = fitz.open(ruta_pdf)\n",
        "    contenido = \"\"\n",
        "    for pagina in doc:\n",
        "        contenido += pagina.get_text()\n",
        "    doc.close()\n",
        "    return contenido\n",
        "\n",
        "def scrape_job_pages(urls: list) -> dict:\n",
        "    scraped = {}\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
        "    }\n",
        "    for url in urls:\n",
        "        try:\n",
        "            resp = requests.get(url, headers=headers, timeout=10)\n",
        "            resp.raise_for_status()\n",
        "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "            text = soup.get_text(separator=\" \", strip=True)\n",
        "            scraped[url] = text\n",
        "        except Exception as e:\n",
        "            scraped[url] = f\"Error: {e}\"\n",
        "    return scraped\n",
        "\n",
        "def build_job_description_prompt(job):\n",
        "    \"\"\"Builds a prompt that generates a professional job description from job data.\"\"\"\n",
        "    return f\"\"\"\n",
        "    You are given the following job information:\n",
        "\n",
        "    - Job Position: \"{job.get('title', '')}\"\n",
        "    - Company: \"{job.get('company', '')}\"\n",
        "    - Location: \"{job.get('location', '')}\"\n",
        "    - Education: \"{job.get('education', '')}\"\n",
        "    - Responsibilities: {', '.join(job.get('responsibilities', []))}\n",
        "    - Skills: {', '.join(job.get('skills', []))}\n",
        "\n",
        "    Task:\n",
        "    Generate a SINGLE, well-written `job_description` in plain, professional English.\n",
        "    - Use the position, company, location, education, responsibilities, and skills provided.\n",
        "    - Maintain a natural, human-readable style.\n",
        "    - Do NOT invent any details beyond what's given.\n",
        "\n",
        "    Example format:\n",
        "    \"The position is for a [Job Position] at [Company], located in [Location], requiring a candidate with an educational background in [Education]. The role involves key responsibilities such as [Responsibility 1], [Responsibility 2], and [Responsibility N]. The required skills for this role include [Skill 1], [Skill 2], and [Skill N].\"\n",
        "\n",
        "    Now generate the `job_description`.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def filter_url(list_jobs):\n",
        "  N = len(list_jobs)\n",
        "  list_results = []\n",
        "  for idx in range(N):\n",
        "    response = client.chat.completions.create(\n",
        "      model=DEPLOYMENT,  # o el modelo que tengas en Azure\n",
        "      temperature=0.3,\n",
        "      max_tokens=800,\n",
        "      messages=[\n",
        "          {\"role\": \"system\", \"content\": \"You are a job filtering and summarizing assistant.\"},\n",
        "          {\"role\": \"user\", \"content\": build_job_description_prompt(list_jobs[idx])}\n",
        "      ]\n",
        "    )\n",
        "    result = response.choices[0].message.content\n",
        "    list_results.append(result)\n",
        "\n",
        "  return list_results\n",
        "\n",
        "\n",
        "def predict(cv_desc, jb_desc, model):\n",
        "    embedding1 = model.encode(cv_desc, convert_to_tensor=True)\n",
        "    embedding2 = model.encode(jb_desc, convert_to_tensor=True)\n",
        "\n",
        "    return util.cos_sim(embedding1, embedding2)\n",
        "\n",
        "def test_all(cv_ruth: str, ruth_model, max_urls):\n",
        "        model = SentenceTransformer(ruth_model)\n",
        "        cv_text = pdf_a_string(cv_ruth)\n",
        "        area_job, cv_information = generate_linkedin_query(cv_text)\n",
        "\n",
        "        print(f\"Generated Query: {area_job}\")\n",
        "        print(f\"Generated cv: {cv_information}\")\n",
        "\n",
        "        links = search_jobs_multilingual(area_job, max_urls, localization=\"Buenos Aires\", antiguedad_maxima='mes')\n",
        "\n",
        "        print(\"Links encontrados:\",links)\n",
        "\n",
        "        data_job = []\n",
        "        summary_dob = []\n",
        "\n",
        "        for url in links:\n",
        "          content_map = asyncio.run(scrape_and_summarize(url))\n",
        "          data_job.append(content_map[\"job\"])\n",
        "          summary_dob.append(content_map[\"summary\"])\n",
        "\n",
        "        print(content_map[\"job\"])\n",
        "        print(\"ahora si\")\n",
        "        print(content_map[\"summary\"])\n",
        "        result_filter = filter_url(list_jobs=data_job)\n",
        "\n",
        "        score_cv_jd = []\n",
        "\n",
        "        for job_desc in result_filter:\n",
        "          score_cv_jd.append(predict(cv_information, job_desc, model))\n",
        "\n",
        "        max_score = max(score_cv_jd)\n",
        "\n",
        "        max_index = score_cv_jd.index(max_score)\n",
        "\n",
        "        print(\"El maximo puesto tiene un score de:\", max_score)\n",
        "        print(\"Link del puesto:\", links[max_index])\n",
        "        print(\"Resumen del puesto:\", summary_dob[max_index])\n",
        "\n",
        "# https://in.linkedin.com/jobs/view/ai-ml-engineer-at-oneseven-tech-ost-4241281880\n",
        "# bueno"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LggLLexzGTan"
      },
      "outputs": [],
      "source": [
        "test_all(cv_ruth=\"/content/drive/MyDrive/ApplAI/Curriculum_Tiziano_Martin.pdf\", ruth_model=\"/content/drive/MyDrive/ApplAI/modelsSave/mini_finetuned_Allmini\", max_urls=2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
